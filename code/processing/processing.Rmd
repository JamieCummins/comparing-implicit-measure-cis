---
title: "Data processing"
author: "Jamie Cummins & Ian Hussey"
date: "`r Sys.Date()`"
output: html_document
---

# TODO

- ensure scores for each measure are calculated correctly, eg using evaluations rather than RTs for AMP
- why norm bootstraps rather than BCA? It generally shows best performance. however, reviewers might ask for multiple ones to show robustness, thats what happened with the irap paper.

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

```{r}

library(tidyverse)
library(parallel)
library(boot)

raw_trial_level_df <- read.delim("../../data/raw/impraw.txt") 
raw_sessions_df <- read.delim("../../data/raw/allds.txt")

```

# screen and identify the completed sessions and their corresponding measures

```{r}

completed_sessions_df <- raw_sessions_df |>
  select(User_ID, SESSION_ID, 
         contains(c("Race", "Pltc", "Self")), 
         -contains(c("21", "22", "31", "32", "33", 
                     "RT", "cgat"))) |>
  mutate(across(everything(),
                as.character)) |>
  pivot_longer(cols = -contains(c("ID")),
               names_to = "scale",
               values_to = "score") %>%
  filter(!is.na(score),
         scale != "race") |>
  separate(scale, 
           c("measure", "domain"),
           "(?!^)(?=[[:upper:]])")

```

# tidy the trial-level data and include only those completed cases

```{r}

trial_level_tasks_labelled_df <-
  raw_trial_level_df |>
  mutate(across(everything(),
                as.character)) |>
  
  # remove sessions with no complete implicit measures
  semi_join(completed_sessions_df, by = "SESSION_ID") |>
  mutate(measure_firstblock = case_when(TASK_NAME_S %in% c("BBlkBad", "BWhtGood",
                                                           "BRepBad", "BDemGood",
                                                           "BOtrBad", "BSlfGood") ~ "biat_con",
                                        TASK_NAME_S %in% c("BBlkGood", "BWhtBad",
                                                           "BRepGood", "BDemBad",
                                                           "BOtrGood", "BSlfBad") ~ "biat_incon",
                                        TASK_NAME_S %in% c("GBlkBad", "GWhtGood",
                                                           "GRepBad", "GDemGood",
                                                           "GOtrBad", "GSlfGood") ~ "gnat_con",
                                        TASK_NAME_S %in% c("GBlkGood", "GWhtBad",
                                                           "GRepGood", "GDemBad",
                                                           "GOtrGood", "GSlfBad") ~ "gnat_incon",
                                        TASK_NAME_S %in% c("eppltc", 
                                                           "eprace", "epself") ~ "ept",
                                        TASK_NAME_S %in% c("amppltc", 
                                                           "amprace", "ampself") ~ "amp",
                                        TASK_NAME_S %in% c("iwhtgood", 
                                                           "iDemGood", "iSlfGood") ~ "iat_con",
                                        TASK_NAME_S %in% c("iwhtbad", 
                                                           "iRepGood", "iSlfBad") ~ "iat_incon",
                                        TASK_NAME_S %in% c("scbdblk", "scbdotr",
                                                           "scbdrep", "scgddem",
                                                           "scgdslf", "scgdwht") ~ "siat_con",
                                        TASK_NAME_S %in% c("scgdblk", "scgdotr",
                                                           "scgdrep", "scbddem",
                                                           "scbdslf", "scbdwht") ~ "siat_incon",
                                        TASK_NAME_S %in% c("spdpltc", "spdrace",
                                                           "spdself") ~ "speededresponse",
                                        TRUE ~ "irrelevant"),
         domain = case_when(str_detect(TASK_NAME_S, "Blk|Wht|blk|wht|race") ~ "race",
                            str_detect(TASK_NAME_S, "Slf|Otr|slf|otr|self") ~ "self",
                            str_detect(TASK_NAME_S, "Dem|Rep|dem|rep|pltc") ~ "politics",
                            TRUE ~ "irrelevant")) |>
  filter(domain != "irrelevant",
         measure_firstblock != "irrelevant") |>
  separate(measure_firstblock,
           into = c("measure", "first_block"),
           sep = "_")


```


# score measures
To-do: make sure measures are scored with appropriate metric (i.e., AMP with accuracy not RT)

```{r}

block_consistent_terms <- "White People/Good|Self/Good|Democrats/Good|Good Words/White|Bad Words/Black|Good Words/Democrats|Bad Words/Republicans|Good Words/Self|Bad Words/Others"
block_inconsistent_terms <- "Black People/Good|Others/Good|Republicans/Good|Good Words/Black|Bad Words/White|Good Words/Republicans|Bad Words/Democrats|Good Words/Others|Bad Words/Self"
trial_consistent_terms <- "mtmmgore|mtmmkry|mtmmclnt|mtmmobm|mtmmbclnt|mtmmhclnt|Mine|Me|Myself|Self|\\[I\\]|epwm|epwf|mtmmwf|mtmmwm"
trial_inconsistent_terms <- "mtmmrgn|mtmmbsh|mtmmgln|mtmmcnd|They|Them|Their|Others|epbm|epbf|mtmmbf|mtmmbm"


critical_trials_df <- trial_level_tasks_labelled_df |>
  
  # keep only relevant blocks/trials
  filter(
    (measure == "iat" & BLOCK_NUMBER %in% c("2", "3", "5", "6"))
    |
    (measure == "biat" & BLOCK_NUMBER %in% c(1:8) & TRIAL_NUMBER %in% c(4:19))
    |
    (measure == "amp" & BLOCK_NUMBER %in% c(1:2) & str_detect(TRIAL_NAME_S, "[XXXXXX]|Grey", negate = TRUE))
    |
    (measure == "gnat" & BLOCK_NUMBER %in% c(1:8))
    |
    (measure == "ept" & BLOCK_NUMBER %in% c(1:3))
    |
    (measure == "siat")) |>
  mutate(trial_consistency = case_when(
    measure %in% c("iat", "biat", "gnat") & 
      str_detect(BLOCK_PAIRING_DEFINITION_S, 
                 block_consistent_terms) ~ "consistent",
    measure %in% c("iat", "biat", "gnat") & 
      str_detect(BLOCK_PAIRING_DEFINITION_S, 
                 block_inconsistent_terms) ~ "inconsistent",
    measure == "amp" & str_detect(TRIAL_NAME_S, 
                                  trial_consistent_terms) ~ "consistent",
    measure == "amp" & str_detect(TRIAL_NAME_S, 
                                  trial_inconsistent_terms) ~ "inconsistent",
    measure == "siat" & 
      str_detect(BLOCK_PAIRING_DEFINITION_S, 
                 block_consistent_terms) ~ "consistent",
    measure == "siat" & 
      str_detect(BLOCK_PAIRING_DEFINITION_S, 
                 block_inconsistent_terms) ~ "inconsistent",
    measure == "ept" & 
      TRIAL_RESPONSE_S == "Good" & 
      str_detect(TRIAL_NAME_S, 
                 trial_consistent_terms) ~ "consistent",
    measure == "ept" & 
      TRIAL_RESPONSE_S == "Bad" & 
      str_detect(TRIAL_NAME_S,
                 trial_inconsistent_terms) ~ "consistent",
    measure == "ept" & 
      TRIAL_RESPONSE_S == "Good" & 
      str_detect(TRIAL_NAME_S, 
                 trial_inconsistent_terms) ~ "inconsistent",
    measure == "ept" & 
      TRIAL_RESPONSE_S == "Bad" & 
      str_detect(TRIAL_NAME_S, 
                 trial_consistent_terms) ~ "inconsistent"),
    TRIAL_ERROR = as.numeric(TRIAL_ERROR),
    TRIAL_LATENCY = as.numeric(TRIAL_LATENCY),
    score = case_when(measure %in% c("iat", "biat", "siat", "gnat") ~ TRIAL_LATENCY,
                      measure %in% c("amp") ~ TRIAL_ERROR,
                      measure %in% c("ept") ~ abs(TRIAL_ERROR - 1)))

view_df <- critical_trials_df |>
  filter(measure == "ept")

```


# apply exclusions

## at session level
Exclusions applied same as those from Bar-Anan & Nosek, 2014

```{r}

participants_after_session_level_exclusions <- critical_trials_df |>
  mutate(rt_under_300 = case_when(TRIAL_LATENCY < 300 ~ 1,
                                   TRUE ~ 0)) |>
  group_by(SESSION_ID, measure, domain) |>
  summarise(mean_rts_under_300 = mean(rt_under_300),
            mean_errors = mean(TRIAL_ERROR)) |>
  ungroup() |>
  filter(
    (measure %in% c("iat", "biat", "siat", "gnat") & mean_rts_under_300 < .1) 
    |
    (measure %in% c("ept") & mean_errors < .4) # errors are coded as 1
    | 
    (measure %in% c("amp") & (mean_errors < .95 & mean_errors > .05))
  ) |>
  select(SESSION_ID, measure, domain)
  
```

## at trial level

Exclusions applied same as those from Bar-Anan & Nosek, 2014

```{r}

all_exclusions_applied_df <- critical_trials_df |>
  semi_join(participants_after_session_level_exclusions) |>
  filter(
    (measure %in% c("iat", "biat", "siat") & (TRIAL_LATENCY > 400 & TRIAL_LATENCY < 10000)) 
    |
    (measure %in% c("gnat") & (TRIAL_LATENCY > 400 & TRIAL_LATENCY < 1200) & (TRIAL_ERROR %in% c(0, 3)))
    |
    (measure %in% c("ept", "amp"))
  )

all_exclusions_applied_df |>
  filter(measure == "ept")

```

# compute scores

```{r}

n_boots = 100
subset_sample = TRUE # to reduce processing time for testing

# load models if already calculated
# file.exists("models/pi_scores.rds")
if(FALSE) {
  
  pi_scores <- read_rds("../../data/processed/data_processed.rds")

} else {
  
  # Fast calculation of the A statistic - code from Ruscio (2008) supplementary materials
  PI_score <- function(data, i) {
    data_with_indexes <- data[i,] # boot function requires data and index
    x  <- na.omit(data_with_indexes$score[data_with_indexes$trial_consistency == "inconsistent"])
    y  <- na.omit(data_with_indexes$score[data_with_indexes$trial_consistency == "consistent"])
    nx <- length(x)
    ny <- length(y)
    rx <- sum(rank(c(x, y))[1:nx])
    PI <- (rx / nx - (nx + 1) / 2) / ny
    return(PI)
  }
  
  bootstrap_PI_score <- function(data){
    
    require(dplyr)
    require(boot)
    
    fit <- 
      boot::boot(data      = data, 
                 statistic = PI_score, 
                 R         = n_boots, 
                 sim       = "ordinary", 
                 stype     = "i",
                 parallel  = "multicore", 
                 ncpus     = parallel::detectCores())
    
    results <- boot::boot.ci(fit, conf = 0.95, type = c("norm"))
    
    output <- 
      tibble(method   = c("normal"),
             estimate = fit$t0,
             ci_lower = c(results$normal[2]),
             ci_upper = c(results$normal[3]))
    
    return(output)
  }
  
  
  if(subset_sample == TRUE) {
    trials_to_process <- all_exclusions_applied_df |>
      slice_head(n = 500000)   
    } else {
    trials_to_process <- all_exclusions_applied_df
  }
  
  # bootstrap PI scores 
  pi_scores <- trials_to_process |>
    mutate(session_id = as.factor(SESSION_ID),
           trial_consistency = as.character(trial_consistency)) |>
    
    # ensure a sufficient number of observations per person per measure per trial type
    group_by(session_id, measure, domain, trial_consistency) |>
    filter(n() >= 3) |>
    
    # remove those who do not have sufficient observations for both trials per measure
    group_by(session_id, domain, measure) |>
    filter(n_distinct(trial_consistency) == 2) |>
    
    # do bootstrapping scores
    group_by(session_id, measure, domain) |>
    do(bootstrap_PI_score(data = .)) |>
    ungroup() |>
    mutate(sig = ifelse((ci_lower < 0.50 & ci_upper < 0.50) | (ci_lower > 0.50 & ci_upper > 0.50), TRUE, FALSE),
           ci_width = ci_upper - ci_lower) 
  
  # save to disk
  write_rds(pi_scores, "../../data/processed/data_processed.rds")
  
}

```

# plot distributions 

```{r}

pi_scores |>
  ggplot() +
  aes(forcats::fct_reorder(session_id, estimate),
      estimate,
      color = sig) +
  geom_pointrange(aes(ymin = ci_lower,
                    ymax = ci_upper)) +
  geom_hline(yintercept = .5) +
    theme_classic() +
  theme(axis.ticks.x = element_blank(),
        axis.text.x = element_blank()) +
  facet_grid(domain ~ measure,
             scales = "free_x")


pi_scores |>
  group_by(measure, domain) |>
  summarise(proportion_sig = mean(sig, na.rm = TRUE))

```

